A neural network is a machine learning program, or model, that makes decisions in a manner similar to the human brain, by using processes that mimic the way biological neurons work together to identify phenomena, weigh options and arrive at conclusions.
Every neural network consists of layers of nodes or artificial neurons, an input layer, one or more hidden layers, and an output layer.
Each node connects to others, and has its own associated weight and threshold.
If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network.
Otherwise, no data is passed along to the next layer of the network.
Neural networks rely on training data to learn and improve their accuracy over time.
Once they are fine-tuned for accuracy, they are powerful tools in computer science and artificial intelligence, allowing us to classify and cluster data at a high velocity.
Tasks in speech recognition or image recognition can take minutes versus hours when compared to the manual identification by human experts.
One of the best-known examples of a neural network is Googles search algorithm.
Neural networks are sometimes called artificial neural networks or simulated neural networks.
They are a subset of machine learning, and at the heart of deep learning models.
Think of each individual node as its own linear regression model, composed of input data, weights, a bias, and an output.
Once an input layer is determined, weights are assigned.
These weights help determine the importance of any given variable, with larger ones contributing more significantly to the output compared to other inputs.
All inputs are then multiplied by their respective weights and then summed.
Afterward, the output is passed through an activation function, which determines the output.
If that output exceeds a given threshold, it fires the node, passing data to the next layer in the network.
This results in the output of one node becoming in the input of the next node.
This process of passing data from one layer to the next layer defines this neural network as a feedforward network.
In this instance, you would go surfing; but if we adjust the weights or the threshold, we can achieve different outcomes from the model.
When we observe one decision, like in the above example, we can see how a neural network could make increasingly complex decisions depending on the output of previous decisions or layers.
In the example above, we used perceptrons to illustrate some of the mathematics at play here, but neural networks leverage sigmoid neurons, which are distinguished by having values between zero and one.
Since neural networks behave similarly to decision trees, cascading data from one node to another, having x values between zero and one will reduce the impact of any given change of a single variable on the output of any given node, and subsequently, the output of the neural network.
As we start to think about more practical use cases for neural networks, like image recognition or classification, well leverage supervised learning, or labeled datasets, to train the algorithm.
As we train the model, well want to evaluate its accuracy using a cost function.
This is also commonly referred to as the mean squared error.
Ultimately, the goal is to minimize our cost function to ensure correctness of fit for any given observation.
As the model adjusts its weights and bias, it uses the cost function and reinforcement learning to reach the point of convergence, or the local minimum.
The process in which the algorithm adjusts its weights is through gradient descent, allowing the model to determine the direction to take to reduce errors (or minimize the cost function).
With each training example, the parameters of the model adjust to gradually converge at the minimum.
Most deep neural networks are feedforward, meaning they flow in one direction only, from input to output.
However, you can also train your model through backpropagation; that is, move in the opposite direction from output to input.
Backpropagation allows us to calculate and attribute the error associated with each neuron, allowing us to adjust and fit the parameters of the model(s) appropriately.
Neural networks can be classified into different types, which are used for different purposes.
While this isnt a comprehensive list of types, the below would be representative of the most common types of neural networks that youll come across for its common use cases:
Feedforward neural networks, or multi-layer perceptrons, are what weve primarily been focusing on within this article.
They are comprised of an input layer, a hidden layer or layers, and an output layer.
While these neural networks are also commonly referred to as MLPs, its important to note that they are actually comprised of sigmoid neurons, not perceptrons, as most real-world problems are nonlinear.
Data usually is fed into these models to train them, and they are the foundation for computer vision, natural language processing, and other neural networks.
Convolutional neural networks are similar to feedforward networks, but theyre usually utilized for image recognition, pattern recognition, and/or computer vision.
These networks harness principles from linear algebra, particularly matrix multiplication, to identify patterns within an image.
Recurrent neural networks are identified by their feedback loops.
These learning algorithms are primarily leveraged when using time-series data to make predictions about future outcomes, such as stock market predictions or sales forecasting.
Deep Learning and neural networks tend to be used interchangeably in conversation, which can be confusing.
As a result, its worth noting that the deep in deep learning is just referring to the depth of layers in a neural network.
A neural network that consists of more than three layers, which would be inclusive of the inputs and the output, can be considered a deep learning algorithm.
A neural network that only has two or three layers is just a basic neural network.
